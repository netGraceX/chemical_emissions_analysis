{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import openpyxl as op\n",
    "from openpyxl import load_workbook\n",
    "import xlrd\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excel to CSV conversion\n",
    "def read_files(dir_path, store_path):\n",
    "    os.chdir(dir_path)\n",
    "    file_names = [i for i in glob.glob('*.{}'.format('xlsx'))]   \n",
    "    for f in file_names:\n",
    "        read_file = pd.read_excel (dir_path+f)        \n",
    "        read_file.to_csv(store_path+Path(f).stem+'.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve data from DB\n",
    "def retrieve_data(db_name, username, db_host, db_password, db_port, query, column):\n",
    "    conn = psycopg2.connect(dbname=db_name, user=username, host=db_host, password=db_password, port=db_port)\n",
    "    cur = conn.cursor(column, cursor_factory=psycopg2.extras.DictCursor)    \n",
    "    cur.execute(query)\n",
    "    df = cur.fetchall()\n",
    "    return df                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__add_calculated_column(df)__\n",
    "\n",
    "Check which measurement unit was already present in the file (µg/m3 or ppb) and add a column for the one not present, calculating the values with the correct formula.\n",
    "\n",
    "FORMULAS\n",
    "- molecular_weight x ppb_concentration = 24,45 x µg/m3_concentration\n",
    "- µg_m3_concentration = (peso molecular_weight x ppb_concentration) ÷ 24,45\n",
    "- ppb_concentration = (24,45 x µg_m3_concentration/m3) ÷ molecular_weight \n",
    "\n",
    "ASSUMPTIONS\n",
    "- molecular_weight [g/mol]\n",
    "- 24,45 is the volume (l) of a mole of gas when the temperature is 25°C and the pression in 1 atm(1 atm = 1,01325 bar).\n",
    "\n",
    "ONLINE CONVERTER TO CHECK RESULTS\n",
    "https://www.wkcgroup.com/tools-room/micrograms-per-cubic-meter-parts-per-billion-converter/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_calculated_column(df):\n",
    "    dic_mol_weights = {'TRS':122.14, 'C6H6':78.11, 'H2S' =34.1}\n",
    "    column_names = ['TRS_ug/m3','C6H6_ug/m3', 'H2S_ug/m3']\n",
    "    \n",
    "    for column in column_names:         \n",
    "        if (column in list(df.columns)):   \n",
    "            chemical = column.replace('_ug/m3','')\n",
    "            df.insert(loc=df.columns.get_loc('column'), column=column.replace('_ug/m3', '_ppb'), value=((24.45*df[column])/dic_mol_weights[chemical]))    \n",
    "        else:\n",
    "            chemical = column.replace('_ppb','')\n",
    "            df.insert(loc=df.columns.get_loc(column)+1, column=column.replace('_ppb', '_ug/m3'), value=((dic_mol_weights[chemical]*df[column])/24.45))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**add_id_column(df, file_name)**\n",
    "\n",
    "Add id column related to the control unit, based on the file name (the file name contains the control unit name).\n",
    "\n",
    "- ATM05 --> id = 1\n",
    "- ATM06 --> id = 2\n",
    "- ATM07 --> id = 3\n",
    "- ATM10 --> id = 4\n",
    "- ATM11 --> id = 5\n",
    "- ATM12 --> id = 6\n",
    "- ATM13 --> id = 7\n",
    "- ATM14 --> id = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_id_column(df, file_name):  \n",
    "    id_dic = {'ATM05':1, 'ATM06':2, 'ATM07':3, 'ATM10':4, 'ATM11':5, 'ATM12':6,'ATM13':7, 'ATM14':8}\n",
    "    for atm in dic.keys():\n",
    "        if atm in file_name: \n",
    "            df.insert(0, column='atm_id', value=(id_dic[atm]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning - 2021 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**merge_date_time_2021(df)**\n",
    "\n",
    "Merging date and time in the same column for 2021 data.\n",
    "\n",
    "Pandas reads time as float because the corresponding column was stored with '.' instead of ':'.\n",
    "Perform the following steps:\n",
    "- number to string conversion\n",
    "- adding 0 at the beginning or end hour, e.g.:\n",
    "    - 0.15 --> 00.15, \n",
    "    - 10.5 --> 10.50, \n",
    "    - 1.1 --> 01.10\n",
    "- merging date and time in the same column\n",
    "- removing time column\n",
    "- replacing '.' with ':' for string to datetime conversion\n",
    "- string to datetime conversion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_date_time_2021(df):\n",
    "    column_names = list(df.columns)    \n",
    "    if 'Data' in str(column_names[0]) and 'ora' in str(column_names[1]):                                               \n",
    "        for hour in df['ora']:\n",
    "            str_hour = str(hour)\n",
    "            if(len(str_hour) is 4 and str_hour[1] is '.'):                 \n",
    "                new_hour = str_hour.zfill(5)    \n",
    "                df['ora'] = df['ora'].replace(hour,new_hour)\n",
    "            elif (len(str_hour) is 4 and str_hour[2] is '.'):\n",
    "                new_hour = str_hour+'0'  \n",
    "                df['ora'] = df['ora'].replace(hour,new_hour)\n",
    "            elif (len(str_hour) is 3 and str_hour[1] is '.'):\n",
    "                new_hour = str_hour.zfill(4) + '0'\n",
    "                df['ora'] = df['ora'].replace(hour,new_hour)          \n",
    "        df[\"Data\"] = (df[\"Data\"].apply(str)+' '+df[\"ora\"].apply(str))                     \n",
    "        df.drop('ora', axis=1, inplace = True)   \n",
    "        df[\"Data\"] = df[\"Data\"].apply(lambda x: x.replace(\".\", \":\"))                \n",
    "        df['Data'] =  pd.to_datetime(df['Data'], infer_datetime_format=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete all the unnecessary values in the 'Data' column (there are 'Data' values that don't have \n",
    "#corresponding chemical values).\n",
    "def drop_date_values(df):\n",
    "    column_names = list(df.columns) \n",
    "    row_index = df[df['ora'].isnull()].index.tolist()   \n",
    "    for column in column_names:\n",
    "        if(len(row_index) != 0 and column not in 'Data' and column not in 'ora'):            \n",
    "            df = df.iloc[:row_index[0]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tipology1(df):\n",
    "    df = pd.read_excel(f, sheet_name=1)            \n",
    "    df.drop('postazione', axis=1, inplace = True)            \n",
    "    df = drop_date_values(df)\n",
    "    df = merge_date_time_2021(df) \n",
    "    df.set_axis(['data', 'trs_ppb', 'trs_stato', 'voc_ppm', 'voc_stato', 'c6h6_ug_m3', 'c6h6_stato', 'h2s_ug_m3', 'h2s_stato', 'H2SJ_ug/m3', 'H2SJ_stato','pidvoc_ppb', 'pidvoc_stato'], axis=1, inplace=True)                \n",
    "    column_list = [sheet.cell(0,column).value for column in range(sheet.ncols)]            \n",
    "    if 'H2SJ_ug/m3' in column_list:                \n",
    "        df.drop('H2SJ_ug/m3', axis=1, inplace = True)\n",
    "        df.drop('H2SJ_stato', axis=1, inplace = True)      \n",
    "    else:     \n",
    "        df.drop('H2SJ_ppb', axis=1, inplace = True)\n",
    "        df.drop('H2SJ_stato', axis=1, inplace = True)  \n",
    "    df = add_calculated_column(df) \n",
    "    add_id_columns(df, f)\n",
    "    #df.to_excel(store_path+Path(f).stem+\".xls\", index=False)        \n",
    "    df.to_csv(store_path+Path(f).stem+'.csv', index = None, header=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tipology2(df):\n",
    "    df = pd.read_excel(f, sheet_name=1, headers=0, skiprows=5)   \n",
    "    df = drop_date_values(df)\n",
    "    df['Data'] = df['Data'].fillna(method='ffill')\n",
    "    df = merge_date_time_2021(df)                      \n",
    "    column_list = [sheet.cell(5,column).value for column in range(sheet.ncols)]            \n",
    "    if 'ug/m3' in column_list:   \n",
    "        df.set_axis(['data', 'trs_ppb', 'trs_stato', 'voc_ppm', 'voc_stato', 'c6h6_ug_m3', 'c6h6_stato', 'h2s_ug_m3', 'h2s_stato', 'H2SJ_ug/m3', 'H2SJ_stato','pidvoc_ppb', 'pidvoc_stato'], axis=1, inplace=True)                \n",
    "        df.drop('H2SJ_ug/m3', axis=1, inplace = True)\n",
    "        df.drop('H2SJ_stato', axis=1, inplace = True)                       \n",
    "    else:                \n",
    "        df.set_axis(['data', 'trs_ppb', 'trs_stato', 'voc_ppm', 'voc_stato', 'c6h6_ppb', 'c6h6_stato', 'h2s_ppb', 'h2s_stato', 'H2SJ_ppb', 'H2SJ_stato', 'pidvoc_ppb', 'pidvoc_stato'], axis=1, inplace=True)                            \n",
    "        df.drop('H2SJ_ppb', axis=1, inplace = True)\n",
    "        df.drop('H2SJ_stato', axis=1, inplace = True)                    \n",
    "    df = add_calculated_column(df)   \n",
    "    add_id_columns(df, f)\n",
    "    #df.to_excel(store_path+Path(f).stem+\".xls\", index=False)                     \n",
    "    df.to_csv(store_path+Path(f).stem+'.csv', index = None, header=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__clean_2021_data(dir_path, store_path)__\n",
    "\n",
    "_xlrd_ necessary because 2021 files are in _xls_ format.\n",
    "\n",
    "The only sheet needed is _'Report settimanale'_.\n",
    "\n",
    "There are two types of files; to dynamically check which one we are using, we check \n",
    "if the first row has the word _\"Date\"_ in it, which is not present in the other type.\n",
    "\n",
    "Steps performed according to type:\n",
    " - Type 1: Delete the \"postazione\" column because it is not needed.\n",
    " - Type 2:\n",
    "    * Ignored the first 5 rows (unnecessary information).\n",
    "    * added the missing values for the date column.\n",
    "    * added header because some columns are unnamed due to bad reading of the file. 2 types of header because in some cases there were errors\n",
    "      in units _ppb_ instead of _ug/m3_ and vice versa\n",
    "    \n",
    "Shared steps (type 1 and 2):\n",
    "  -  *merge_date_time_2021*\n",
    "  - deleted columns for _H2SJ_ because these sensors were broken, so data are incorrect\n",
    "  -  *add_calculated_column*\n",
    "  -  *add_id_column*\n",
    "  - converted file to .csv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_2021_data(dir_path, store_path):    \n",
    "    os.chdir(os.path.abspath(dir_path))    \n",
    "    file_names = [i for i in glob.glob('*.{}'.format('xls'))]       \n",
    "    for f in file_names:        \n",
    "        wb = xlrd.open_workbook(f);\n",
    "        sheet = wb.sheet_by_name('Report settimanale')           \n",
    "        df = pd.DataFrame()         \n",
    "        if \"Data\" in sheet.cell(0, 1).value:                \n",
    "            dff = clean_tipology1(df)\n",
    "        else:            \n",
    "            dff = clean_tipology2(df)\n",
    "    return dff  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dff = clean_2021_data(\"../DATASET_2021/ATM05/\", \"../CLEANED_DATA_2021/ATM05/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning - 2022 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**merge_date_time_2022(df)**\n",
    "\n",
    "Merging date and time in the same column for 2022 data.\n",
    "\n",
    "_'data'_ and _'ora'_ were already in the correct format, the only two steps performed were:\n",
    "merging date and time in the same column\n",
    "- removing time column\n",
    "- string to datetime conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_date_time_2022(df):\n",
    "    column_names = list(df.columns)     \n",
    "    if 'data' in str(column_names[0]) and 'ora' in str(column_names[1]):                                                               \n",
    "        df[\"data\"] = (df[\"data\"].apply(str)+' '+df[\"ora\"].apply(str))                     \n",
    "        df.drop('ora', axis=1, inplace = True)                      \n",
    "        df['data'] =  pd.to_datetime(df['data'], infer_datetime_format=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__clean_2022_data(dir_path, store_path)__\n",
    "\n",
    "Steps performed:\n",
    "- *merge_date_time_2022*\n",
    "- deleted columns for _H2SJ_ because these sensors were broken, so data are incorrect\n",
    "- conversion of columns containing numeric values to _float_ because their type was originally _object_ and therefore it was not possible to perform the mathematical operations required by the _add_calculated_column_ function\n",
    "- changed the header to match that of 2021\n",
    "- replacedstrings *** used in place of empty cells with NaN \n",
    "- _add_calculated_column_\n",
    "- _add_id_column_\n",
    "- converted to .csv \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_2022_data(dir_path, store_path):\n",
    "    os.chdir(os.path.abspath(dir_path))    \n",
    "    file_names = [i for i in glob.glob('*.{}'.format('csv'))]     \n",
    "    df = pd.DataFrame() \n",
    "    for f in file_names:               \n",
    "        df = pd.read_csv(f, sep='\\t')             \n",
    "        df.drop('H2SJ', axis=1, inplace = True)\n",
    "        df.drop('Status.4', axis=1, inplace = True)\n",
    "        df = df.replace('***', np.nan)\n",
    "        column_names = list(df.columns)\n",
    "        for column in column_names:\n",
    "            if 'Date' not in column and 'Ora' not in column and 'Status' not in column:\n",
    "                df[column] = df[column].astype(float)               \n",
    "        df.set_axis(['data', 'ora', 'trs_ppb', 'trs_stato', 'voc_ppm', 'voc_stato', 'c6h6_ug_m3', 'c6h6_stato', 'h2s_ug_m3', 'h2s_stato','pidvoc_ppb', 'pidvoc_stato', 'Unnamed'], axis=1, inplace=True)\n",
    "        df.drop('Unnamed', axis=1, inplace=True)                         \n",
    "        df = merge_date_time_2022(df)                \n",
    "        df = add_calculated_column(df) \n",
    "        add_id_column(df, f)\n",
    "        df.to_csv(store_path+Path(f).stem+'.csv', index = None, header=True)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = clean_2022_data(\"../DATASET_2022/ATM05/\", \"../CLEANED_DATA_2022/ATM05/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning - meteo dataset 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**clean_meteo_data_22(dir_path, store_path)**\n",
    "\n",
    "Steps performed:\n",
    "\n",
    "- chaged the headers to improve readability (e.g., winddirection --> wind_direction)\n",
    "- replaced the 'T' character present in the dates with a blank space so that the format is similar to that of the other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_meteo_data_22(dir_path, store_path):\n",
    "    os.chdir(os.path.abspath(dir_path))    \n",
    "    file_names = [i for i in glob.glob('*.{}'.format('csv'))]     \n",
    "    df = pd.DataFrame() \n",
    "    for f in file_names:                       \n",
    "        df = pd.read_csv(f, sep=',') \n",
    "        df.set_axis(['date', 'wind_speed', 'wind_direction', 'temperature', 'radiation', 'pressure', 'precipitation', 'humidity'], axis=1, inplace=True)\n",
    "        #df = clean_meteo_date_22(df)\n",
    "        column_names = list(df.columns)     \n",
    "        if 'date' in str(column_names[0]):        \n",
    "            df[\"date\"] = df[\"date\"].apply(lambda x: x.replace(\"T\", \" \"))\n",
    "            df.to_csv(store_path+Path(f).stem+'.csv', index = None, header=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = clean_meteo_data_22(\"../METEO_DATA/2022_METEO_DATA/\", \"../CLEANED_METEO_DATA/CLEANED_METEO_2022/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different data formats, 'DD/MM/YYYY' in some file, 'MM/DD/YYYY' in others. \n",
    "It become a problem when there were data out of the available range.\n",
    "Considering 2021 data, we have data between 04/12/2021 and 11/28/2021 in the MM/DD/YYYY format, so if there is 03/11/2021 it is considered a March data, instead in the correct format it is a November data, and March isn't in the available range at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_min_data(data, month, day):\n",
    "    if(data.month < month or (data.month == month and data.day < day)):\n",
    "        return True\n",
    "    \n",
    "def check_max_data(data, month, day):\n",
    "    if(data.month > month or data.month == day):\n",
    "        return True\n",
    "\n",
    "def change_data_format(db_name, username, db_password, db_port, query, column, file_path):\n",
    "    df = retrieve_data(db_name, username, db_password, db_port, query, column)\n",
    "    df = pd.DataFrame(np.array(df), columns = ['atm_id', 'data', 'trs_ppb', 'trs_ug_m3', 'trs_stato', 'voc_ppm', 'voc_stato', 'c6h6_ppb', 'c6h6_ug_m3', 'c6h6_stato', 'h2s_ppb', 'h2s_ug_m3', 'h2s_stato', 'pidvoc_ppb', 'pidvoc_stato'])\n",
    "    df['data'] = pd.to_datetime(df['data'], format = '%Y-%m-%d %H:%M:%S')           \n",
    "    for elem in range(0, len(df['data'])):          \n",
    "        if(df.loc[elem].at[\"data\"].year == 2021 and (check_min_data(df.loc[elem].at[\"data\"], 4, 12) or check_max_data(df.loc[elem].at[\"data\"], 11, 28))):            \n",
    "            date = df.loc[elem].at[\"data\"].strftime('%Y-%d-%m %H:%M:%S')              \n",
    "            df.at[elem,'data'] = date\n",
    "        elif(df.loc[elem].at[\"data\"].year == 2022 and (check_min_data(df.loc[elem].at[\"data\"], 6, 20) or df.loc[elem].at[\"data\"].month > 8)):                          \n",
    "            date = df.loc[elem].at[\"data\"].strftime('%Y-%d-%m %H:%M:%S')                \n",
    "            df.at[elem,'data'] = date\n",
    "    df = df.sort_values(by='data') \n",
    "    df.to_csv(file_path, index = None, header=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_list = ['atm05', 'atm06', 'atm07', 'atm10', 'atm11', 'atm12', 'atm13', 'atm14']\n",
    "for atm in atm_list:\n",
    "    query = 'SELECT * FROM ' + atm\n",
    "    change_data_format(db_name, username, password, db_port, query, 'data', '../cleaned_data/cleaned_db.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
