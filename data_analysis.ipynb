{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import kats\n",
    "from kats.utils.decomposition import TimeSeriesDecomposition\n",
    "from kats.detectors.trend_mk import MKDetector\n",
    "from kats.consts import TimeSeriesData\n",
    "from kats.detectors.seasonality import FFTDetector\n",
    "from kats.detectors.cusum_detection import CUSUMDetector\n",
    "from kats.detectors.outlier import OutlierDetector\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data(db_name, username, db_host,  db_password, db_port, query, column):\n",
    "    conn = psycopg2.connect(dbname=db_name, user=username, host=db_host, password=db_password, port=db_port)\n",
    "    cur = conn.cursor(column, cursor_factory=psycopg2.extras.DictCursor)    \n",
    "    cur.execute(query)\n",
    "    df = cur.fetchall()\n",
    "    return df                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataframe based on year\n",
    "def split_df(df):\n",
    "    temp1 = []    \n",
    "    temp2 = []\n",
    "    for elem in range(0, len(df['data'])):        \n",
    "        if(df.loc[elem].at[\"data\"].year == 2021):\n",
    "            temp1.append(df.loc[elem])\n",
    "        elif(df.loc[elem].at[\"data\"].year == 2022):\n",
    "            temp2.append(df.loc[elem])\n",
    "    df1 = pd.DataFrame(temp1).sort_values(by='data') \n",
    "    df2 = pd.DataFrame(temp2).sort_values(by='data') \n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global and Weekly Null Values Count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_null_values(db_name, username, db_host, db_password, db_port, query, column, column_name):\n",
    "    df = retrieve_data(db_name, username, db_host, db_password, db_port, query, column)\n",
    "    df = pd.DataFrame(np.array(df), columns = ['data', column_name])\n",
    "    df['data'] = pd.to_datetime(df['data'], format = '%Y-%m-%d %H:%M:%S', utc=True)\n",
    "    df.set_index('data', inplace = True)  \n",
    "    print()\n",
    "    print(column_name)\n",
    "    print()   \n",
    "    null_values = df[column_name].isna().sum()\n",
    "    print('Missing values:', null_values)\n",
    "    percent_missing_values = null_values * 100 / len(df[column_name])\n",
    "    print('Percentage of missing values:', percent_missing_values)    \n",
    "    not_null_values = df[column_name].notna().sum()\n",
    "    print('Not missing values:', not_null_values)\n",
    "    nd_values = df[df[column_name] == 'ND'].shape[0]\n",
    "    print('ND values:', nd_values)\n",
    "    percent_nd_values = nd_values * 100 / len(df[column_name])\n",
    "    print('Percentage of ND values:', percent_nd_values)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count values for all sensors\n",
    "atm_list = ['atm05', 'atm06', 'atm07', 'atm10', 'atm11', 'atm12', 'atm13', 'atm14']\n",
    "columns = ['trs_ppb', 'trs_stato', 'voc_ppm', 'voc_stato', 'c6h6_ppb', 'c6h6_stato', 'h2s_ppb', 'h2s_stato', 'pidvoc_ppb', 'pidvoc_stato']\n",
    "for atm in atm_list:\n",
    "    for column in columns:\n",
    "        count_null_values(db_name, username, host, password, port, 'SELECT data, ' + column + ' FROM ' + atm, 'data', column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_null_by_week(db_name, username, db_password, db_port, query,column, column_name):\n",
    "    df = retrieve_data(db_name, username, db_password, db_port, query, column)        \n",
    "    df = pd.DataFrame(np.array(df), columns = ['data', 'null_values', 'not_null_values'])\n",
    "    df['data'] = pd.to_datetime(df['data'], format = '%Y-%m-%d %H:%M:%S', utc=True)\n",
    "    #print(df['data'].count())\n",
    "    df['percentage_null_values'] = ((df['null_values'] / (df['null_values'] + df['not_null_values']))*100)    \n",
    "    for value in df['percentage_null_values']: \n",
    "        df['percentage_null_values'] = df['percentage_null_values'].replace(value, round(value, 2))    \n",
    "    df1, df2 = split_df(df)\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show or save weekly null values \n",
    "def show_and_save_weekly_nulls(df, save_path, atm, column, year):  \n",
    "    print()\n",
    "    print('Chemical: ', column)        \n",
    "    print('Values less than 10% - 2021: ', len(df[df['percentage_null_values'] < 10]))\n",
    "    print()\n",
    "    print('Values less than 10% - 2022: ', len(df[df['percentage_null_values'] < 10]))\n",
    "    print()\n",
    "    save_path = '../Notebook/Results/Null_Values' + atm.upper()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    t = ax.table(cellText = df.values, colWidths = [0.9]*len(df.columns),  colLabels=df.columns,  loc='center', cellLoc = 'center')\n",
    "    t.auto_set_font_size(False) \n",
    "    t.set_fontsize(10) \n",
    "    plt.show()\n",
    "    plt.savefig(save_path + year + column + '.jpg', bbox_inches='tight')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly ND Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nd_by_week(db_name, username, db_host , db_password,db_port, query_nd, query_not_nd, column, column_name):\n",
    "    df = retrieve_data(db_name, username, db_host, db_password, db_port, query_nd, column)        \n",
    "    df = pd.DataFrame(np.array(df), columns = ['data', 'nd_values'])\n",
    "    df['data'] = pd.to_datetime(df['data'], format = '%Y-%m-%d %H:%M:%S', utc=True)    \n",
    "    df_temp = retrieve_data(db_name, username, db_host, db_password, db_port, query_not_nd, column)\n",
    "    df_temp = pd.DataFrame(np.array(df_temp), columns = ['data', 'not_nd_values'])\n",
    "    df = df.join(df_temp['not_nd_values'])        \n",
    "    df['percentage_nd_values'] = ((df['nd_values'] / (df['nd_values'] + df['not_nd_values']))*100)    \n",
    "    for value in df['percentage_nd_values']: \n",
    "        df['percentage_nd_values'] = df['percentage_nd_values'].replace(value, round(value, 2))\n",
    "    df1, df2 = split_df(df)\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_and_save_weekly_nd(df, save_path, atm, column, year):     \n",
    "    print()\n",
    "    print('Chemical': column)\n",
    "    print(len(df['percentage_nd_values']))\n",
    "    print('Values less than 10% - '+ year + ': ', len(df[df['percentage_nd_values'] < 10]))\n",
    "    print()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    t = ax.table(cellText = df.values, colWidths = [0.9]*len(df.columns),  colLabels=df.columns,  loc='center', cellLoc = 'center')\n",
    "    t.auto_set_font_size(False) \n",
    "    t.set_fontsize(10)\n",
    "    #ax.set_xlabel('Tabella valori nulli ' + column_name)\n",
    "    #fig.tight_layout()    \n",
    "    plt.show()\n",
    "    plt.savefig(save_path + year + atm + column + '.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nulls and ND function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_list = ['atm05', 'atm06', 'atm07', 'atm10', 'atm11', 'atm12', 'atm13', 'atm14']\n",
    "columns = ['trs_ppb', 'trs_stato', 'voc_ppm', 'voc_stato', 'c6h6_ppb', 'c6h6_stato', 'h2s_ppb', 'h2s_stato', 'pidvoc_ppb', 'pidvoc_stato']\n",
    "for atm in atm_list:\n",
    "    save_path_nulls = '../Notebook/Results/Nulls_ND_Values/Nulls_Weekly/nulls_tables' + atm.upper()\n",
    "    save_path_nd = '../Notebook/Results/Nulls_ND_Values/Nd_Weekly/nd_tables' + atm.upper()\n",
    "    for column in columns:\n",
    "        #null values\n",
    "        query_nulls = \"select time_bucket('1 week', data) as bucket, sum(case when \" + column + \" is null then 1 else 0 end) null_values,count(\" + column + \") not_nulls from \" + atm + \" GROUP BY bucket\"                    \n",
    "        df1_nulls, df2_nulls = count_null_by_week(db_name, username, host, password, port, query, 'data', column)            \n",
    "        show_and_save_weekly_nulls(df1_nulls, save_path_nulls, atm, column, '2021')\n",
    "        show_and_save_weekly_nulls(df2_nulls, save_path_nulls, atm, column, '2022')\n",
    "        #nd values\n",
    "        query_nd = \"select time_bucket('1 week', data) as bucket, count(*) as nd_values from \" + atm + \" WHERE \" +  column + \" = 'ND' GROUP BY bucket \"              \n",
    "        query_not_nd = \"select time_bucket('1 week', data) as bucket, count(*) as not_nd_values from \" + atm + \" WHERE \" +  column + \" <> 'ND' GROUP BY bucket \"      \n",
    "        df1_nd, df2_nd = count_nd_by_week(db_name, username, host, password, port, query_nd, query_not_nd 'data', column)\n",
    "        show_and_save_weekly_nd(df1_nd, save_path_nd, atm, column, '2021')\n",
    "        show_and_save_weekly_nd(df2_nd, save_path_nd, atm, column, '2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global charts results showed after getting percentage results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Charts of percentage of null values by chemical detection\n",
    "labels = ['ATM05', 'ATM06', 'ATM07', 'ATM10', 'ATM11', 'ATM12', 'ATM13', 'ATM14']\n",
    "values = [9, 11.4, 5.6, 6.7, 6.7, 8.6, 12.7, 7.4]\n",
    "plt.bar(labels, values)\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.title(\"Percentage of null values by chemical detection\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Charts of percentage of null values by detection status\n",
    "labels = ['ATM05', 'ATM06', 'ATM07', 'ATM10', 'ATM11', 'ATM12', 'ATM13', 'ATM14']\n",
    "null_values = [0.67, 4.6, 0.85, 0.68, 0.68, 0.62, 1.12, 0.68]\n",
    "nd_values = [8.3, 6.8, 4.7, 6, 6, 8, 11.6, 6.7]\n",
    "x = np.arange(len(labels))\n",
    "width = 0.4\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, null_values, width, label='Valori nulli')\n",
    "rects2 = ax.bar(x + width/2, nd_values, width, label='Valori ND')\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_title('Percentage null and ND values by detection status')\n",
    "ax.set_xticks(x, labels)\n",
    "ax.legend()\n",
    "ax.bar_label(rects1, padding=3)\n",
    "ax.bar_label(rects2, padding=3)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values above and below maximum and minimum sensors thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting lower or higher values than detectable by the sensor - Global\n",
    "def count_vs_treshold(db_name, username, db_host, db_password, db_port, query, cur_column, column_name, min_value, max_value):\n",
    "    df = retrieve_data(db_name, username, db_host, db_password, db_port, query, cur_column)    \n",
    "    df = pd.DataFrame(np.array(df), columns = ['data', column_name])\n",
    "    df['data'] = pd.to_datetime(df['data'], format = '%Y-%m-%d %H:%M:%S', utc=True)\n",
    "    df.set_index('data', inplace = True)\n",
    "    df[column_name] = df[column_name].astype(float)    \n",
    "    count_min_values = len(df[df[column_name] < min_value])\n",
    "    percent_min_values = count_min_values * 100 / len(df[column_name])\n",
    "    count_max_values = len(df[df[column_name] > max_value])\n",
    "    percent_max_values = count_max_values * 100 / len(df[column_name])\n",
    "    print()\n",
    "    print(column_name)\n",
    "    print('Values below minimum threshold: ', count_min_values)\n",
    "    print('Percentage values below minimum threshold: ', percent_min_values)\n",
    "    print('Values above maximum threshold', count_max_values)\n",
    "    print('Percentage values above maximum threshold: ', percent_max_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Charts of percentage of values below minimun threshold (there aren't values above thresholds)\n",
    "def plot_below_thresholds(labels, values, name):\n",
    "    fig, ax = plt.subplots()        \n",
    "    bars = ax.bar(labels, values)\n",
    "    ax.bar_label(bars)\n",
    "    plt.ylabel(\"Percentage %\")\n",
    "    plt.title(\"Percentage of values below minimun threshold \" + name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = ['trs_ppb', 'voc_ppm', 'c6h6_ppb', 'h2s_ppb', 'pidvoc_ppb']\n",
    "plot_below_thresholds(labels, [0, 90, 51.87, 38.21, 11.73], 'ATM05', )\n",
    "plot_below_thresholds(labels, [0, 88.6, 42.18, 25.77, 8.5], 'ATM06')\n",
    "plot_below_thresholds(labels, [0, 93.54, 53, 0.002, 32.32], 'ATM07')\n",
    "plot_below_thresholds(labels, [0, 82, 26.93, 0, 21.32], 'ATM10')\n",
    "plot_below_thresholds(labels, [0, 91.36, 57.55, 24.33, 8.2], 'ATM11')\n",
    "plot_below_thresholds(labels, [0, 90, 22.54, 21, 19.56], 'ATM12')\n",
    "plot_below_thresholds(labels, [0, 86.38, 19.29, 27, 29.42], 'ATM13')\n",
    "plot_below_thresholds(labels, [0, 87, 31.29, 0, 9.14], 'ATM14')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['ATM05', 'AMT06', 'ATM07', 'ATM10', 'ATM11', 'ATM12', 'ATM13', 'ATM14']\n",
    "plot_below_thresholds(labels, [90, 88.6, 93.54, 82, 91.36, 90, 86.38, 87], 'voc_ppm')\n",
    "plot_below_thresholds(labels, [51.87, 42.18, 53, 26.93, 57.55, 22.54, 19.29, 31.29], 'c6h6_ppb')\n",
    "plot_below_thresholds(labels, [38.21, 25.77, 0.002, 0,  24.33, 21, 27, 0], 'h2s_ppb')\n",
    "plot_below_thresholds(labels, [11.74, 8.5, 32.32, 21.32, 8.2, 19.56, 29.42, 9.14], 'pidvoc_ppb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_weekly_vs_threshold(db_name, username, db_host, db_password, db_port, column, column_name, min_value, max_value, atm):\n",
    "    query = \"select time_bucket('1 week', data) as bucket, COUNT(*) AS total_values, COUNT(CASE WHEN \" + column_name + \" < \" + str(min_value) + \" THEN 1 END) AS min_values, COUNT(CASE WHEN \" + column_name + \" > \" + str(max_value) + \" THEN 1 END) AS max_values from \" + atm + \" GROUP BY bucket\"\n",
    "    df = retrieve_data(db_name, username, db_host, db_password, db_port, query, column)        \n",
    "    df = pd.DataFrame(np.array(df), columns = ['data', 'total_values', 'min_values', 'max_values',])                         \n",
    "    df['data'] = pd.to_datetime(df['data'], format = '%Y-%m-%d %H:%M:%S', utc=True)                            \n",
    "    df['min_percentage'] = ((df['min_values'] / (df['total_values'])*100))    \n",
    "    df['max_percentage'] = ((df['max_values'] / (df['total_values'])*100))    \n",
    "    for value in df['min_percentage']: \n",
    "        df['min_percentage'] = df['min_percentage'].replace(value, round(value, 2))\n",
    "    for value in df['max_percentage']: \n",
    "        df['max_percentage'] = df['max_percentage'].replace(value, round(value, 2))\n",
    "    df1, df2 = split_df(df)\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekly_below_charts(df, save_path, year, atm, column):\n",
    "    fig, ax = plt.subplots()        \n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    bars = ax.bar(df['data'], df['min_percentage'])\n",
    "    ax.bar_label(bars)\n",
    "    ax.set_xlabel('Week')\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_title('Weekly percentage values below the threshold' + column)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(save_path + year + atm + column + '.jpg', bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekly_below_table(df, save_path, year, atm, column):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    t = ax.table(cellText = df.values, colWidths = [0.9]*len(df.columns),  colLabels=df.columns,  loc='center', cellLoc = 'center')\n",
    "    t.auto_set_font_size(False) \n",
    "    t.set_fontsize(10)\n",
    "    plt.savefig(save_path + year + atm + column + '.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['trs_ppb', 'voc_ppm', 'c6h6_ppb', 'h2s_ppb', 'pidvoc_ppb']\n",
    "atm_names = ['atm05', 'atm06', 'atm07', 'atm10', 'atm11', 'atm12', 'atm13', 'atm14']\n",
    "dic_min_threshold = {\"trs_ppb\" : 0, \"voc_ppm\" : 0.6, \"c6h6_ppb\" : 0.1, \"h2s_ppb\" : 2, \"pidvoc_ppb\" : 0}\n",
    "dic_max_threshold = {\"trs_ppb\" : 10000, \"voc_ppm\" : 25, \"c6h6_ppb\" : 30, \"h2s_ppb\" : 3000, \"pidvoc_ppb\" : 40000}\n",
    "\n",
    "for atm in atm_names:\n",
    "    path_weekly_charts = '../Notebook/Results/Values_VS_Thresholds/chart' + atm.upper()\n",
    "    path_weekly_table = '../Notebook/Results/Values_VS_Thresholds/chart' + atm.upper()\n",
    "    for column in columns:\n",
    "        #global count\n",
    "        count_vs_treshold(db_name, username, host, password, port, 'SELECT data, ' + column + ' FROM ' + atm, 'data', column, dic_min_threshold[column], dic_max_threshold[column])\n",
    "        #weekly count\n",
    "        df1, df2 = count_weekly_vs_threshold(db_name, username, host, password, port, 'data', column, dic_min_threshold[column], dic_max_threshold[column], atm)            \n",
    "        weekly_below_charts(df1, path_weekly_charts, '2021', atm, column)\n",
    "        weekly_below_charts(df2, path_weekly_charts, '2022', atm, column)\n",
    "        weekly_below_table(df1, path_weekly_table, '2021', atm, column)\n",
    "        weekly_below_table(df2, path_weekly_table, '2022', atm, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekly Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekly_average(db_name, username, db_host, db_password, db_port, query, cur_column, column_name):\n",
    "    df = retrieve_data(db_name, username, db_host, db_password, db_port, query, cur_column)    \n",
    "    df = pd.DataFrame(np.array(df), columns = ['data', 'weekly_average'])\n",
    "    df['data'] = pd.to_datetime(df['data'], format = '%Y-%m-%d %H:%M:%S', utc=True)\n",
    "    #df.set_index('data', inplace = True)\n",
    "    #df[column_name] = df[column_name].astype(float)\n",
    "    df.sort_index(inplace = True)     \n",
    "    df1, df2 = split_df(df)\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_and_save_weekly_average(df, save_path, year):    \n",
    "    fig, ax = plt.subplots()  \n",
    "    plt.plot(df['data'], df['weekly_average'])\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    ax.set_xlabel('Week')\n",
    "    ax.set_ylabel('Average')\n",
    "    ax.set_title('Weekly average ' + column)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(save_path + year + atm + column + '.jpg'+ column + '.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_list = ['atm05', 'atm06', 'atm07', 'atm10', 'atm11', 'atm12', 'atm13', 'atm14']\n",
    "columns = ['trs_ppb', 'voc_ppm', 'c6h6_ppb','h2s_ppb', 'pidvoc_ppb']\n",
    "for atm in atm_list:    \n",
    "    for column in columns:\n",
    "        save_path = '../Notebook/Results/Weekly_Average/weekly_average' + atm.upper()\n",
    "        query = \"select time_bucket('1 week', data) as bucket, avg( \" + column + \") FROM \" + atm + \" GROUP BY bucket\"\n",
    "        df1, df2 = weekly_average(db_name, username, host, password, port, query, 'data', column)\n",
    "        show_and_save_weekly_average(df1, save_path, '2021')\n",
    "        show_and_save_weekly_average(df2, save_path, '2022')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers Analysis\n",
    " - Outliers\n",
    " - Outliers compared to Weekly Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(df):\n",
    "    df_temp = df.rename(columns={\"data\":\"time\"})    \n",
    "    df_temp = TimeSeriesData(df_temp)            \n",
    "    outlier_detector = OutlierDetector(df_temp, \"additive\")\n",
    "    outlier_detector.detector()\n",
    "    outliers = outlier_detector.outliers              \n",
    "    df_out1 = []            \n",
    "    for data in df.index:                 \n",
    "        if df1.loc[data].at['data'] in outliers[0]:                       \n",
    "            df_out.append(df.loc[data])  \n",
    "                    \n",
    "    if len(df_out) != 0:\n",
    "        df_out = pd.DataFrame(np.array(df_out), columns = ['data', column])\n",
    "    return df_out            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_detection (column, atm):\n",
    "    query = 'SELECT data, ' + column + ' FROM ' + atm            \n",
    "    df = retrieve_data(db_name, username, db_host, db_password, db_port, query, cur_column)    \n",
    "    df pd.DataFrame(np.array(df), columns = ['data', column_name])\n",
    "    df['data'] = pd.to_datetime(df['data'], format = '%Y-%m-%d %H:%M:%S', utc=True)    \n",
    "    df[column_name] = df[column_name].astype(float)\n",
    "    df.interpolate(method ='ffill', limit_direction='forward', inplace=True)\n",
    "    df1, df2 = split_df(df) \n",
    "    df1_out = outliers(df1)\n",
    "    df2_out = outliers(df2)\n",
    "    return df1_out, df2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_outliers_to_file(df, save_path, file_name):\n",
    "    with open(save_path + file_name, \"a\") as f:\n",
    "        print(atm + ' ' + column, file=f)\n",
    "        print( file=f)                                \n",
    "        outlier_detector = OutlierDetector(df, \"additive\")\n",
    "        outlier_detector.detector()\n",
    "        outliers = outlier_detector.outliers\n",
    "        print(outliers[0], file=f)\n",
    "        print( file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_chart(df):\n",
    "    fig, ax = plt.subplots()  \n",
    "    #fig.set_size_inches(18, 10)                 \n",
    "    ax.scatter(df_out['data'], df_out[column], alpha=1.0)\n",
    "    ax.set_title('Outliers ' + column)\n",
    "    ax.set_xlabel('Week')\n",
    "    ax.set_ylabel('Outlier')\n",
    "    plt.xticks(rotation=45) \n",
    "    #plt.show()\n",
    "    #plt.savefig(save_path + atm.upper() + ' outliers ' + column + '2021' + '.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_chart_vs_average(df, df_average):\n",
    "    fig, ax = plt.subplots(2)  \n",
    "    fig.set_size_inches(18, 10)                 \n",
    "    ax[0].plot(df_average['data'], df_average['weekly_average'])   \n",
    "    ax[0].set_title('Weekly average ' + column)\n",
    "    ax[0].set_xlabel('Week')\n",
    "    ax[0].set_ylabel('Average')\n",
    "    ax[1].scatter(df_out['data'], df_out[column], alpha=1.0)\n",
    "    ax[1].set_title('Outliers ' + column)\n",
    "    ax[1].set_xlabel('Week')\n",
    "    ax[1].set_ylabel('Outlier')\n",
    "    plt.xticks(rotation=45) \n",
    "    #plt.show()\n",
    "    #plt.savefig(save_path + atm.upper() + ' outliers ' + column + '2021' + '.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outliers charts\n",
    "columns = ['trs_ppb', 'voc_ppm', 'c6h6_ppb', 'h2s_ppb', 'pidvoc_ppb']\n",
    "atm_names = ['atm05', 'atm06', 'atm07', 'atm10', 'atm11', 'atm12', 'atm13', 'atm14']\n",
    "for atm in atm_names:\n",
    "        for column in columns:\n",
    "            df1_out, df2_out = outliers_detection (column, atm)\n",
    "            outliers_chart(df1_out)\n",
    "            outliers_chart(df2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outliers compared to weekly average charts\n",
    "columns = ['trs_ppb', 'voc_ppm', 'c6h6_ppb', 'h2s_ppb', 'pidvoc_ppb']\n",
    "atm_names = ['atm05', 'atm06', 'atm07', 'atm10', 'atm11', 'atm12', 'atm13', 'atm14']\n",
    "for atm in atm_names:\n",
    "    for column in columns:\n",
    "        query_average = \"select time_bucket('1 week', data) as bucket, avg( \" + column + \") FROM \" + atm + \" GROUP BY bucket\"\n",
    "        df1_average, df2_average = weekly_average(db_name, username, host, password, port, query_average, 'data', column)\n",
    "        df1_out, df2_out = outliers_detection (save_path, file_name1, file_name2)\n",
    "        outliers_chart_vs_average(df1_out, df1_average)\n",
    "        outliers_chart_vs_average(df2_out, df2_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save outliers to .txt\n",
    "columns = ['trs_ppb', 'voc_ppm', 'c6h6_ppb', 'h2s_ppb', 'pidvoc_ppb']\n",
    "atm_names = ['atm05', 'atm06', 'atm07', 'atm10', 'atm11', 'atm12', 'atm13', 'atm14']\n",
    "for atm in atm_names:\n",
    "        for column in columns:\n",
    "            df1_out, df2_out = outliers_detection (column, atm)\n",
    "            save_outliers_to_file(df1_out, '../Notebook/Results/', 'Outliers 2021')\n",
    "            save_outliers_to_file(df2_out, '../Notebook/Results/', 'Ouliers 2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality Analysis\n",
    " - Seasonality with statsmodel\n",
    " - seasonality with kats\n",
    " - change detection kats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save seasonality charts as jpg \n",
    "def seasonality_statsmodel_plot(df, column_name, atm, year):    \n",
    "    y = df[column_name]\n",
    "    #y.plot(title=column_name);\n",
    "    seasonal_decomp = seasonal_decompose(y, model=\"additive\", period=56)\n",
    "    fig, axes = plt.subplots(4, 1, sharex=True)\n",
    "    fig.set_size_inches(25.5, 20.5)\n",
    "    seasonal_decomp.observed.plot(ax=axes[0], legend=False, color='r')\n",
    "    axes[0].set_ylabel('Observed')\n",
    "    seasonal_decomp.trend.plot(ax=axes[1], legend=False, color='g')\n",
    "    axes[1].set_ylabel('Trend')\n",
    "    seasonal_decomp.seasonal.plot(ax=axes[2], legend=False)\n",
    "    axes[2].set_ylabel('Seasonal')\n",
    "    seasonal_decomp.resid.plot(ax=axes[3], legend=False, color='k')\n",
    "    axes[3].set_ylabel('Residual')\n",
    "    plt.savefig(save_path + atm.upper() + 'seasonality ' + column + year + '.jpg', bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonality_analisys_statsmodel(save_path, db_name, username, db_host, db_password, db_port, query, cur_column, column_name):    \n",
    "    df = retrieve_data(db_name, username, db_host, db_password, db_port, query, cur_column)    \n",
    "    df = pd.DataFrame(np.array(df), columns = ['data', column_name])\n",
    "    df['data'] = pd.to_datetime(df['data'], format = '%Y-%m-%d %H:%M:%S', utc=True)\n",
    "    #df.set_index('data', inplace = True)\n",
    "    df[column_name] = df[column_name].astype(float)\n",
    "    '''\n",
    "    print('Summary')\n",
    "    print('')\n",
    "    print(df.describe())\n",
    "    print()\n",
    "    print('Null values')\n",
    "    print()\n",
    "    print(df.isnull().sum())\n",
    "    '''\n",
    "    df.interpolate(method ='ffill', limit_direction='forward', inplace=True)\n",
    "    df1, df2 = split_df(df)\n",
    "    df1.set_index('data', inplace = True)\n",
    "    df2.set_index('data', inplace = True)\n",
    "    \n",
    "    seasonality_statsmodel_plot(df1, column_name, atm, '2021')\n",
    "    seasonality_statsmodel_plot(df2, column_name, atm, '2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['trs_ppb', 'voc_ppm', 'c6h6_ppb', 'h2s_ppb', 'pidvoc_ppb']\n",
    "atm_names = ['atm05', 'atm06', 'atm07', 'atm10', 'atm11', 'atm12', 'atm13', 'atm14']\n",
    "for atm in atm_names:\n",
    "    for column in columns:\n",
    "        query = 'SELECT data, ' + column + ' FROM ' + atm        \n",
    "        seasonality_analisys_statsmodel('../Analysis/seasonality_statsmodel/', db_name, username, host, password, port, query, 'data', column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonality_decomposition(df, year):\n",
    "    df = df.rename(columns={\"data\":\"time\"})    \n",
    "    df = TimeSeriesData(df)\n",
    "    decompose = TimeSeriesDecomposition(df1, decomposition='additive', method='STL')\n",
    "    _ = decompose.decomposer()\n",
    "    decompose.plot()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(save_path + atm.upper() + 'seasonality ' + column + year + '.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seasonality analysis with kats\n",
    "def seasonality_kats(db_name, username, db_host, db_password, db_port, query, cur_column, column_name, save_path):    \n",
    "    df = retrieve_data(db_name, username, db_host, db_password, db_port, query, cur_column)    \n",
    "    df = pd.DataFrame(np.array(df), columns = ['data', column_name])\n",
    "    df['data'] = pd.to_datetime(df['data'], format = '%Y-%m-%d %H:%M:%S', utc=True)\n",
    "    #df.set_index('data', inplace = True)\n",
    "    df[column_name] = df[column_name].astype(float)\n",
    "    df.interpolate(method ='ffill', limit_direction='forward', inplace=True)\n",
    "    df1, df2 = split_df(df)\n",
    "    seasonality_decomposition(df1, '2021')\n",
    "    seasonality_decomposition(df2, '2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['trs_ppb', 'voc_ppm', 'c6h6_ppb', 'h2s_ppb', 'pidvoc_ppb']\n",
    "atm_names = ['atm05', 'atm06', 'atm07', 'atm10', 'atm11', 'atm12', 'atm13', 'atm14']\n",
    "for atm in atm_names:\n",
    "    for column in columns:\n",
    "        query = 'SELECT data, ' + column + ' FROM ' + atm\n",
    "        seasonality_kats(db_name, username, host, password, port, query, 'data', column, 'Notebook/Results/')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_seasonality_to_file(df, save_path, file_name, atm, column):\n",
    "    with open(save_path + file_name, \"a\") as f:\n",
    "        print(atm + ' ' + column, file=f)\n",
    "        print( file=f) \n",
    "        fft_detector = FFTDetector(df)\n",
    "        print(fft_detector.detector(), file=f)\n",
    "        print( file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save seasonality info to .txt\n",
    "columns = ['trs_ppb', 'voc_ppm', 'c6h6_ppb', 'h2s_ppb', 'pidvoc_ppb']\n",
    "atm_names = ['atm05', 'atm06', 'atm07', 'atm10', 'atm11', 'atm12', 'atm13', 'atm14']\n",
    "for atm in atm_names:\n",
    "    for column in columns:\n",
    "        query = 'SELECT data, ' + column + ' FROM ' + atm\n",
    "        df1, df2 = seasonality_kats(db_name, username, host, password, port, query, 'data', column)     \n",
    "        save_seasonality_to_file(df1, '../Notebook/Results', 'seasonality2021.txt', atm, column)\n",
    "        save_seasonality_to_file(df2, '../Notebook/Results', 'seasonality2021.txt', atm, column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_detection(df, save_path, year):\n",
    "    detector = CUSUMDetector(df)\n",
    "    change_points = detector.detector(change_directions=[\"increase\", \"decrease\"])        \n",
    "    detector.plot(change_points1)\n",
    "    plt.rcParams[\"figure.figsize\"] = (25,18)\n",
    "    plt.xticks(rotation=45)\n",
    "    #plt.savefig(save_path + atm.upper() + 'change_point ' + column + year + '.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display change detection point charts or save them on disk (comment/uncomment the right line)\n",
    "    columns = ['trs_ppb', 'voc_ppm', 'c6h6_ppb', 'h2s_ppb', 'pidvoc_ppb']\n",
    "    atm_names = ['atm05', 'atm06', 'atm07', 'atm10', 'atm11', 'atm12', 'atm13', 'atm14']\n",
    "    for atm in atm_names:\n",
    "        for column in columns:            \n",
    "            query = 'SELECT data, ' + column + ' FROM ' + atm\n",
    "            df = retrieve_data(db_name, username, host, password, port, query, 'data', column)\n",
    "            df1, df2 = split_df(df)\n",
    "            change_detection(df1, '../Notebook/Results', '2021')\n",
    "            change_detection(df2, '../Notebook/Results', '2022')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
